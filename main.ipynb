{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97372eb4",
   "metadata": {},
   "source": [
    "# Math 4383 Project\n",
    "### Zachary Koenig, Colin Crippen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bfdc1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135c6493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1437, 64) (1437, 10)\n",
      "Validation set shape: (360, 64) (360, 10)\n",
      "One-hot encoded y shape: (1797, 10)\n"
     ]
    }
   ],
   "source": [
    "# --- Load the 8x8 digits dataset ---\n",
    "digits = load_digits()\n",
    "\n",
    "# The dataset contains 8x8 images and their corresponding digit labels.\n",
    "# X contains image data and y contains the target labels.\n",
    "X = digits.images      # Shape: (n_samples, 8, 8)\n",
    "y = digits.target      # Shape: (n_samples,)\n",
    "\n",
    "# --- Data Preprocessing ---\n",
    "\n",
    "# 1. Flatten the images:\n",
    "#    Convert each 8x8 image into a 1D array of 64 pixels.\n",
    "n_samples = X.shape[0]\n",
    "X_flat = X.reshape(n_samples, -1)   # Now shape is (n_samples, 64)\n",
    "\n",
    "# 2. Normalize the inputs:\n",
    "#    The pixel values in the digits dataset are in the range 0-16.\n",
    "#    Dividing by 16 scales them to the range [0, 1].\n",
    "X_normalized = X_flat / 16.0\n",
    "\n",
    "# 3. One-Hot Encode the Labels:\n",
    "#    There are 10 classes (digits 0-9), so create one-hot encoded vectors.\n",
    "num_classes = 10\n",
    "y_onehot = np.eye(num_classes)[y]\n",
    "\n",
    "# --- Split into Training and Verification (Validation) Sets ---\n",
    "# For example, reserve 20% of the data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_normalized, y_onehot, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Verify the shapes\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)  # Should show ~80% of the samples\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)      # Should show ~20% of the samples\n",
    "print(\"One-hot encoded y shape:\", y_onehot.shape)    # Expected: (n_samples, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e30f06",
   "metadata": {},
   "source": [
    "Sigmoid function to enable nonlinear function representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6aa7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Activation functions and their derivatives ---\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Compute the sigmoid activation.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    \"\"\"Compute the derivative of the sigmoid function.\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute the softmax activation for each row of x.\"\"\"\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))  # for numerical stability\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d54b1159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "input_size = 8 * 8   # 64 inputs, one per pixel\n",
    "hidden_size = 64       # You can adjust this\n",
    "output_size = 10       # 10 classes for digits 0-9\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 1_000\n",
    "\n",
    "# --- Initialize Weights and Biases ---\n",
    "np.random.seed(42)  # For reproducibility\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "b2 = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c15f328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 0.7012\n",
      "Epoch 100: loss = 2.3000\n",
      "Epoch 200: loss = 2.2964\n",
      "Epoch 300: loss = 2.2868\n",
      "Epoch 400: loss = 2.2602\n",
      "Epoch 500: loss = 2.1891\n",
      "Epoch 600: loss = 2.0295\n",
      "Epoch 700: loss = 1.7801\n",
      "Epoch 800: loss = 1.5283\n",
      "Epoch 900: loss = 1.3305\n"
     ]
    }
   ],
   "source": [
    "# --- Training Loop ---\n",
    "num_train_samples = X_train.shape[0]  # Use real number of training samples\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass:\n",
    "    Z1 = np.dot(X_train, W1) + b1       # Input to hidden layer\n",
    "    A1 = sigmoid(Z1)                   # Hidden layer activation\n",
    "    Z2 = np.dot(A1, W2) + b2            # Hidden to output layer\n",
    "    A2 = sigmoid(Z2)                   # Output activation\n",
    "\n",
    "    # Compute Cross-Entropy Loss (adding a small constant for numerical stability)\n",
    "    loss = -np.mean(np.sum(y_train * np.log(A2 + 1e-8), axis=1))\n",
    "    \n",
    "       # Backward pass:\n",
    "    # For softmax and cross-entropy, the gradient simplifies:\n",
    "    dZ2 = A2 - y_train\n",
    "    dW2 = np.dot(A1.T, dZ2) / X_train.shape[0]\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / X_train.shape[0]\n",
    "    \n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * sigmoid_deriv(Z1)\n",
    "    dW1 = np.dot(X_train.T, dZ1) / X_train.shape[0]\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / X_train.shape[0]\n",
    "    \n",
    "    # Update the weights and biases using gradient descent:\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: loss = {loss:.4f}\")\n",
    "\n",
    "\n",
    "# Save the trained weights and biases to disk\n",
    "np.save('W1.npy', W1)\n",
    "np.save('b1.npy', b1)\n",
    "np.save('W2.npy', W2)\n",
    "np.save('b2.npy', b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400fb756",
   "metadata": {},
   "source": [
    "Test a single input:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866874f2",
   "metadata": {},
   "source": [
    "# Load the saved weights and biases\n",
    "```python\n",
    "W1 = np.load('W1.npy')\n",
    "b1 = np.load('b1.npy')\n",
    "W2 = np.load('W2.npy')\n",
    "b2 = np.load('b2.npy')\n",
    "\n",
    "x_new = np.load('test')\n",
    "A1_new = sigmoid(np.dot(x_new, W1) + b1)\n",
    "A2_new = sigmoid(np.dot(A1_new, W2) + b2)\n",
    "prediction = np.argmax(A2_new)\n",
    "print(\"Predicted digit:\", prediction)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0a3335d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 78.06%\n"
     ]
    }
   ],
   "source": [
    "# --- Perform Predictions on the Test Set Using the Loaded Weights ---\n",
    "# Forward pass:\n",
    "# 1. Compute activations for the hidden layer.\n",
    "Z1 = np.dot(X_val, W1) + b1   # Linear combination for the hidden layer.\n",
    "A1 = sigmoid(Z1)              # Apply sigmoid activation.\n",
    "\n",
    "# 2. Compute activations for the output layer.\n",
    "Z2 = np.dot(A1, W2) + b2       # Linear combination for the output layer.\n",
    "A2 = sigmoid(Z2)              # Output activations.\n",
    "\n",
    "# The predicted class for each sample is the one with the highest activation.\n",
    "predictions = np.argmax(A2, axis=1)\n",
    "true_labels = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Calculate the accuracy of the model on the test set.\n",
    "accuracy = np.mean(predictions == true_labels)\n",
    "print(\"Test set accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
